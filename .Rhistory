return(theta)
}
GSS <- function(theta, X, y, lambda, grF){
a <- 0
b <- 2
phi <- (1+sqrt(5))/2
eps <- 0.000000001
while(TRUE){
x1 <- b-(b-a)/phi
x2 <- a+(b-a)/phi
y1 <- lrCostFunction((theta+x1*grF), X, y, lambda)
y2 <- lrCostFunction((theta+x2*grF), X, y, lambda)
if(y1>=y2){
a <- x1
} else{
b <- x2
}
if(abs(b-a)<eps){
x <- (a+b)/2
break
}
}
return(x)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
metrics(predictedLabels,trainLabels)
# test the model
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\t10k-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
metrics(predictedLabels,testLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.0000001
grF <- grlrCostFunction(theta, X, y, lambda)
n <- 10
while(TRUE){
Sk <- -grF
buff <- theta
for(i in 1:n){
buff_grF <- grF
buff_Sk <- Sk
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
theta <- buff + alpha*Sk
w <- sum(grF^2)/sum(buff_grF^2)
Sk <- -grF + w*buff_Sk
}
# print(alpha)
#     print(sum((Sk)^2))
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<=eps){
break
}
}
return(theta)
}
classifier <- learnModel(data = trainData, labels = trainLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.00001
grF <- grlrCostFunction(theta, X, y, lambda)
n <- 10
while(TRUE){
Sk <- -grF
buff <- theta
for(i in 1:n){
buff_grF <- grF
buff_Sk <- Sk
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
theta <- buff + alpha*Sk
w <- sum(grF^2)/sum(buff_grF^2)
Sk <- -grF + w*buff_Sk
}
# print(alpha)
#     print(sum((Sk)^2))
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<=eps){
break
}
}
return(theta)
}
classifier <- learnModel(data = trainData, labels = trainLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.00001
grF <- grlrCostFunction(theta, X, y, lambda)
n <- 10
while(TRUE){
Sk <- -grF
buff <- theta
for(i in 1:n){
buff_grF <- grF
buff_Sk <- Sk
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
theta <- buff + alpha*Sk
w <- sum(grF^2)/sum(buff_grF^2)
Sk <- -grF + w*buff_Sk
}
print(alpha)
#     print(sum((Sk)^2))
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<=eps){
break
}
}
return(theta)
}
classifier <- learnModel(data = trainData, labels = trainLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.00001
grF <- grlrCostFunction(theta, X, y, lambda)
n <- 10
while(TRUE){
Sk <- -grF
buff <- theta
for(i in 1:n){
buff_grF <- grF
buff_Sk <- Sk
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
theta <- buff + alpha*Sk
w <- sum(grF^2)/sum(buff_grF^2)
Sk <- -grF + w*buff_Sk
}
print(alpha)
#     print(sum((Sk)^2))
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<=eps){
break
}
}
return(theta)
}
classifier <- learnModel(data = trainData, labels = trainLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.00001
grF <- grlrCostFunction(theta, X, y, lambda)
n <- 10
while(TRUE){
Sk <- -grF
buff <- theta
for(i in 1:n){
buff_grF <- grF
buff_Sk <- Sk
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
print(alpha)
theta <- buff + alpha*Sk
w <- sum(grF^2)/sum(buff_grF^2)
Sk <- -grF + w*buff_Sk
}
print(alpha)
#     print(sum((Sk)^2))
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<=eps){
break
}
}
return(theta)
}
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.00001
grF <- grlrCostFunction(theta, X, y, lambda)
n <- 10
while(TRUE){
Sk <- -grF
buff <- theta
for(i in 1:n){
buff_grF <- grF
buff_Sk <- Sk
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
print(alpha)
theta <- buff + alpha*Sk
w <- sum(grF^2)/sum(buff_grF^2)
Sk <- -grF + w*buff_Sk
}
#     print(sum((Sk)^2))
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<=eps){
break
}
}
return(theta)
}
classifier <- learnModel(data = trainData, labels = trainLabels)
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
metrics(predictedLabels,trainLabels)
# test the model
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\t10k-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
metrics(predictedLabels,testLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.0000001
#   count <- 0
while(TRUE){
#     count <- count+1
buff <- theta
#     print("GD buff")
#     print(sum(buff^2))
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
theta <- buff - alpha*grF
#     print("GD grF")
#     print(sum(grF^2))
#     print("GD alpha")
#     print(alpha)
#     print("GD diff buff and theta")
#     print(sum((buff-theta)^2))
if(sum((buff-theta)^2)<eps){
break
}
}
return(theta)
}
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.0000001
#   while(TRUE){
#     buff <- theta
#
#     grF <- grlrCostFunction(buff, X, y, lambda)
#     alpha <- GSS(buff, X, y, lambda, grF)
#
#     theta <- buff - alpha*grF
#
#     if(sum((buff-theta)^2)<eps){
#       break
#     }
#   }
while(TRUE){
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(theta, X, y, lambda, grF)
theta <- theta - alpha*grF
if(sum((alpha*grF)^2)<eps){
break
}
}
return(theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.0000001
#   while(TRUE){
#     buff <- theta
#
#     grF <- grlrCostFunction(buff, X, y, lambda)
#     alpha <- GSS(buff, X, y, lambda, grF)
#
#     theta <- buff - alpha*grF
#
#     if(sum((buff-theta)^2)<eps){
#       break
#     }
#   }
while(TRUE){
grF <- grlrCostFunction(theta, X, y, lambda)
alpha <- GSS(theta, X, y, lambda, grF)
theta <- theta - alpha*grF
if(sum((alpha*grF)^2)<eps){
break
}
}
return(theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
GradientDescent <- function(theta, X, y, lambda){
eps <- 0.0000001
while(TRUE){
buff <- theta
grF <- grlrCostFunction(buff, X, y, lambda)
alpha <- GSS(buff, X, y, lambda, grF)
theta <- buff - alpha*grF
if(sum((buff-theta)^2)<eps){
break
}
}
return(theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
library("pROC")
source("load_data.R")
source("learnModel.R")
source("lrCostFunction.R")
source("grlrCostfunction.R")
source("testModel.R")
source("metrics.R")
source("GradientDescent.R")
source("GSS.R")
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\GitHub\\LearnMNIST\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\GitHub\\LearnMNIST\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
#
# train a model
# classifier <- learnModel(data = trainData, labels = trainLabels)
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
metrics(predictedLabels,trainLabels)
# test the model
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\GitHub\\LearnMNIST\\t10k-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\GitHub\\LearnMNIST\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
metrics(predictedLabels,testLabels)
