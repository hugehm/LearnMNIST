# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
learnModel <- function(X, y){
lambda <- 0.1
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(theta, X, (y==temp), lambda))
}
return(all_theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
learnModel <- function(data, labels){
lambda <- 0.1
X <- data
y <- labels
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(theta, X, (y==temp), lambda))
}
return(all_theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
A
classifier <- learnModel(data = trainData, labels = trainLabels)
classifier
learnModel <- function(data, labels){
lambda <- 0.1
X <- data
y <- labels
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
#start loop
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(theta, X, (y==temp), lambda))
}
return(all_theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
clc
clear
clear all()
a
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
lrCostFunction <- function(theta, X, y, lambda){
m<-length(y);
#in cost function
res = 1/m * sum(-y*log(sigmoid(X%*%theta))-(1-y)*log(1-sigmoid(X%*%theta)))+lambda/(2*m)*sum(sum(theta[2:end]^2))
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
learnModel <- function(data, labels){
print("In learn model")
lambda <- 0.1
X <- data
y <- labels
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
#start loop
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(theta, X, (y==temp), lambda))
}
return(all_theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
lrCostFunction <- function(theta, X, y, lambda){
print("In cost function")
m<-length(y);
res = 1/m * sum(-y*log(sigmoid(X%*%theta))-(1-y)*log(1-sigmoid(X%*%theta)))+lambda/(2*m)*sum(sum(theta[2:end]^2))
}
learnModel <- function(data, labels){
print("In learn model")
lambda <- 0.1
X <- data
y <- labels
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
print("start loop")
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(theta, X, (y==temp), lambda))
}
return(all_theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
learnModel <- function(data, labels){
print("In learn model")
lambda <- 0.1
X <- data
y <- labels
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
print("start loop")
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(X, (y==temp), lambda))
}
return(all_theta)
}
# This sciprt file contains a frame for learning handwritten digitals from the MNIST dataset
# load training data from files
data <- loadMNISTData("C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-images.idx3-ubyte", "C:\\Users\\Gennadij\\Documents\\LearnMNIST-master\\train-labels.idx1-ubyte")
trainLabels <- data$labels
trainData <- data$data
print(dim(trainData))
print(dim(trainLabels))
# trainingData should be 60000x786,  60000 data and 784 features (28x28), tha matrix trainData has 60000 rows and 784 columns
# trainingLabels should have 60000x1, one class label \in {0,1,...9} for each data.
#uncomment the following 3 lines to see the nth training example and its class label.
# n = 10;
# image( t(matrix(trainData[n, ], ncol=28, nrow=28)), Rowv=28, Colv=28, col = heat.colors(256),  margins=c(5,10))
# print("Class label:"); print(trainLabels[n])
# train a model
classifier <- learnModel(data = trainData, labels = trainLabels)
predictedLabels <- testModel(classifier, trainData)
#calculate accuracy on training data
print("accuracy on training data:\t")
print(sum(predictedLabels == trainLabels)/length(trainLabels))
#calculate the following error metric for each class obtained on the train data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
# test the model
data <- loadMNISTData("C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-images.idx3-ubyte", "C:\\Users\\User\\YandexDisk\\teaching\\advanced_topics_in_machine_learning\\t10k-labels.idx1-ubyte")
testLabels <- data$labels
testData <- data$data
print(dim(testData))
print(dim(testLabels))
#trainingData should be 10000x786,  10000 data and 784 features (28x28), tha matrix trainData has 10000 rows and 784 columns
#trainingLabels should have 10000x1, one class label \in {0,1,...9} for each data.
predictedLabels <- testModel(classifier, testData)
#calculate accuracy
print("accuracy on test data:\t")
print(sum(predictedLabels == testLabels)/length(testLabels))
#calculate the following error metric for each class obtained on the test data:
#Recall, precision, specificity, F-measure, FDR and ROC for each class separately. Use a package for ROC.
learnModel <- function(data, labels){
print("In learn model")
lambda <- 0.1
X <- data
y <- labels
m <- nrow(X)
n <- ncol(X)
num_labels <- 10
all_theta <- matrix(data = 0, nrow = num_labels, ncol = n+1, byrow = T)
X <- cbind(1:m, X)
initial_theta = 0*1:(n+1)
print("start loop")
for (i in 1:num_labels){
if (i==1)
temp <- num_labels
else
temp <- i-1
all_thetap[i,] <- optim(initial_theta, lrCostFunction(initial_theta, X, (y==temp), lambda))
}
return(all_theta)
}
classifier <- learnModel(data = trainData, labels = trainLabels)
